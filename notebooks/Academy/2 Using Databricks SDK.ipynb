{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the DA object components\n",
    "print(f\"Username: {da.username}\")\n",
    "print(f\"Catalog Name: {da.catalog_name}\")\n",
    "print(f\"Schema Name: {da.schema_name}\")\n",
    "print(f\"Working Directory: {da.paths.working_dir}\")\n",
    "print(f\"Dataset Location: {da.paths.datasets}\")\n",
    "print(f\"Secondary Principal: {da.iam.secondary}\")\n",
    "print(f\"Cluster Name: {da.cluster_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record the credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host\n",
    "# Token\n",
    "# HTTP Path\n",
    "DA.get_credentials_with_warehouse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databricks SDF for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient(\n",
    "    host=os.getenv(\"DATABRICKS_HOST\")\n",
    "    , token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "print(\"Available Clusters in this Workspace:\")\n",
    "for c in w.Clusters.list():\n",
    "    print(c.cluster_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \"${DA.schema_name}\" AS Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data catalog\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient(\n",
    "    host=os.getenv(\"DATABRICKS_HOST\")\n",
    "    , token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "# Ensure the catalog exists\n",
    "try:\n",
    "    w.catalogs.get(DA.catalog_name)\n",
    "    print(f\"Catalog {DA.catalog_name} exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Catalog {DA.catalog_name} does not exist. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if schema exists\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient(\n",
    "    host=os.getenv(\"DATABRICKS_HOST\")\n",
    "    , token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "# Ensure the catalog exists\n",
    "try:\n",
    "    w.schemas.get(f\"{DA.catalog_name}.{DA.schema_name}\")\n",
    "    print(f\"Schema {DA.schema_name} exists in catalog {DA.catalog_name}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Schema {DA.schema_name} does not exist in catalog {DA.catalog_name}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring new table\n",
    "import os\n",
    "from databricks import sql\n",
    "\n",
    "# Define table name\n",
    "table_name = \"sample_sdk_delta_table\"\n",
    "\n",
    "# Define the Delta table creation SQL\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {DA.catalog_name}.{DA.schema_name}.{table_name} (\n",
    "    id INT,\n",
    "    name STRING,\n",
    "    age INT\n",
    ") USING DELTA;\n",
    "\"\"\"\n",
    "\n",
    "# Use the SQL API to execute SQL Command\n",
    "try:\n",
    "    # Connect to Databricks SQL endpoint\n",
    "    with sql.connect(\n",
    "        server_hostname=os.getenv(\"DATABRICKS_HOST\"),\n",
    "        http_path=os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "        access_token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Execute the SQL command to create the table\n",
    "            cursor.execute(create_table_sql)\n",
    "            print(f\"Delta Table {table_name} created successfully in schema '{DA.schema_name}' under catalog '{DA.catalog_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Delta table {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring and Managing Data Hierarchy\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient(\n",
    "    host=os.getenv(\"DATABRICKS_HOST\")\n",
    "    , token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "# Examine the catalogs in the data hierarchy\n",
    "print(\"Examining Metastore hierarchy:\")\n",
    "try:\n",
    "    # List all catalogs\n",
    "    catalogs = w.catalogs.list()\n",
    "    for catalog in catalogs:\n",
    "        catalog_name = catalog.name\n",
    "        print(f\"Catalog: {catalog_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error examining data hierarchy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore schema available within each catalog\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient(\n",
    "    host=os.getenv(\"DATABRICKS_HOST\")\n",
    "    , token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "# Examine the catalogs in the data hierarchy\n",
    "print(\"Examining Metastore hierarchy:\")\n",
    "try:\n",
    "    # List all catalogs\n",
    "    catalogs = w.catalogs.list()\n",
    "    for catalog in catalogs:\n",
    "        catalog_name = catalog.name\n",
    "        print(f\"Catalog: {catalog_name}\")\n",
    "        \n",
    "        # List all schemas within the catalog\n",
    "        schemas = w.schemas.list(catalog_name=catalog_name)\n",
    "        for schema in schemas:\n",
    "            schema_name = schema.name\n",
    "            print(f\"  Schema: {schema_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error examining data hierarchy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the tables available with each schema\n",
    "import os\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize the Databricks workspace client\n",
    "w = WorkspaceClient(\n",
    "    host=os.getenv(\"DATABRICKS_HOST\")\n",
    "    , token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    ")\n",
    "\n",
    "# Examine the catalogs in the data hierarchy\n",
    "print(\"Examining Metastore hierarchy:\")\n",
    "try:\n",
    "    # List all catalogs\n",
    "    catalogs = w.catalogs.list()\n",
    "    for catalog in catalogs:\n",
    "        catalog_name = catalog.name\n",
    "        print(f\"Catalog: {catalog_name}\")\n",
    "        \n",
    "        # List all schemas within the catalog\n",
    "        schemas = w.schemas.list(catalog_name=catalog_name)\n",
    "        for schema in schemas:\n",
    "            schema_name = schema.name\n",
    "            print(f\"  Schema: {schema_name}\")\n",
    "            \n",
    "            # List all tables within the schema\n",
    "            tables = w.tables.list(catalog_name=catalog_name, schema_name=schema_name)\n",
    "            for table in tables:\n",
    "                table_name = table.name\n",
    "                print(f\"    Table: {table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error examining data hierarchy: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh rm -rf ./var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks import sql\n",
    "\n",
    "# Define the Delta table name\n",
    "table_name = \"sample_sdk_delta_table\"\n",
    "\n",
    "# Define the Delta table drop SQL\n",
    "drop_table_sql = f\"\"\"\n",
    "DROP TABLE IF EXISTS {DA.catalog_name}.{DA.schema_name}.{table_name};\n",
    "\"\"\"\n",
    "\n",
    "# Use the SQL API to execute SQL Command\n",
    "try:\n",
    "    # Connect to Databricks SQL endpoint\n",
    "    with sql.connect(\n",
    "        server_hostname=os.getenv(\"DATABRICKS_HOST\"),\n",
    "        http_path=os.getenv(\"DATABRICKS_HTTP_PATH\"),\n",
    "        access_token=os.getenv(\"DATABRICKS_TOKEN\")\n",
    "    ) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            # Execute the SQL command to drop the table\n",
    "            cursor.execute(drop_table_sql)\n",
    "            print(f\"Delta Table {table_name} dropped successfully from schema '{DA.schema_name}' under catalog '{DA.catalog_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error dropping Delta table {table_name}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
